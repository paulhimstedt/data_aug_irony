{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code was derived from https://www.steadforce.com/how-tos/lambada-method-how-to-use-data-augmentation-in-nlu"
      ],
      "metadata": {
        "id": "Yfns6Y4oUANv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Housekeeping"
      ],
      "metadata": {
        "id": "_OjOOb2tS-Mc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5_YMszbYZGj",
        "outputId": "13bb81d7-caf3-42a9-cc0e-f580d055cce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=d127f9b89dac86887ac3e76b6e948b8bc7cea253fab104edd40829885ce8d37c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.39.0.tar.gz (25.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from ktrain) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.5.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ktrain) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ktrain) (23.2)\n",
            "Collecting langdetect (from ktrain)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from ktrain) (0.42.1)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (from ktrain) (3.3.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from ktrain) (5.2.0)\n",
            "Collecting syntok>1.3.3 (from ktrain)\n",
            "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
            "Collecting tika (from ktrain)\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.17.0 in /usr/local/lib/python3.10/dist-packages (from ktrain) (4.35.2)\n",
            "Collecting sentencepiece (from ktrain)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras_bert>=0.86.0 (from ktrain)\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting whoosh (from ktrain)\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_bert>=0.86.0->ktrain) (1.23.5)\n",
            "Collecting keras-transformer==0.40.0 (from keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-pos-embd==0.13.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-multi-head==0.29.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-layer-normalization==0.16.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-position-wise-feed-forward==0.8.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-embed-sim==0.10.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-self-attention==0.51.0 (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->ktrain) (2023.3.post1)\n",
            "Requirement already satisfied: regex>2016 in /usr/local/lib/python3.10/dist-packages (from syntok>1.3.3->ktrain) (2023.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (4.66.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->ktrain) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (2023.11.17)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ktrain) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ktrain) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika->ktrain) (67.7.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.17.0->ktrain) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.17.0->ktrain) (4.5.0)\n",
            "Building wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, tika\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.39.0-py3-none-any.whl size=25319737 sha256=b2689a0b781e1708dc6454a270cca8608a1c19fa45f24098b1b3f161f7cade12\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/fd/0a/ef6252223f3d2c49b06d18e71c74caa43bbf4c64a8c183a46e\n",
            "  Building wheel for keras_bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33499 sha256=1c6f5c2b0d5853447f8afd6c9fcfe6d6a19fc0a3952c13f02e533f4190e953b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/0c/04/646b6fdf6375911b42c8d540a8a3fda8d5d77634e5dcbe7b26\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12286 sha256=e0c0cd708e4cb9d386311c728e2d78c922c666211d7d1efb01660e808074c58c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/cb/22/75a0ad376129177f7c95c0d91331a18f5368fd657f4035ba7c\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3943 sha256=10f8183f2bc6ca1c1ba3ab788c1b0b96c99eded0f34160bf42aefb174d8b3402\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/32/c7/fd35d0d1b840a6c7cbd4343f808d10d0f7b87d271a4dbe796f\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4653 sha256=970b49d5677660f13cc67b0ec8ec085300dd5c9aa15bc99bfc7d528f1493842d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/3a/4b/21db23c0cc56c4b219616e181f258eb7c57d36cc5d056fae9a\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14975 sha256=7492466e9fc1848048e9cf04b5cbcf5f79028e2d99eb06412291a9c7aad7fa67\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/23/4b/06d7ae21714f70fcc25b48f972cc8e5e7f4b6b764a038b509d\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6946 sha256=1ce13f74530323ba4197c9a558aa3fa6ecb7dfb983b25d7deec51a69d89bda06\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/07/1b/b1ca47b6ac338554b75c8f52c54e6a2bfbe1b07d79579979a4\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4968 sha256=0ee4170edfbd49d1da60dfd47754fdf44585c7c21d94a37ea961b2178b3e7467\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/6a/04/d1706a53b23b2cb5f9a0a76269bf87925daa1bca09eac01b21\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=d58627d40d294b836bc0192be4c2d964cb5d74661367ed3a342ddb13279a2126\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=8fc607bd9ff6d7b2d795ad45da56b24b9b90db010dfe3134f7822f1563828f8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32622 sha256=a56361b3825855620ccac174611a038f6e0062ef7e46a8c27ec730656f7ebf95\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "Successfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect tika\n",
            "Installing collected packages: whoosh, sentencepiece, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, tika, keras-multi-head, keras-transformer, keras_bert, ktrain\n",
            "Successfully installed keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.39.0 langdetect-1.0.9 sentencepiece-0.1.99 syntok-1.4.4 tika-2.6.0 whoosh-2.7.4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install ktrain\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "MkIJe4y-ZG38"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wget.download('https://raw.githubusercontent.com/paulhimstedt/sarca_test/main/finetune_gpt.py')\n",
        "wget.download('https://raw.githubusercontent.com/iabufarha/iSarcasmEval/main/train/train.En.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EhNxhVbpY0uI",
        "outputId": "3d057b99-8b30-40f0-84c8-26d72fd3c956"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'train.En.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GLOBAL_GENERATE_MAX_LENGTH =50\n",
        "GLOBALGENERATE_TOP_P = 0.92\n",
        "NUMBER_OF_GENERATED_UTTERANCES_PER_INTENT = 300"
      ],
      "metadata": {
        "id": "EI_1U58ATyQJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map labels"
      ],
      "metadata": {
        "id": "cKI8biSXTGzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df_train = pd.read_csv('train.En.csv')\n",
        "\n",
        "# Define mapping\n",
        "def map_sarcasm_to_intent(sarcasm):\n",
        "    return \"sarcastic\" if sarcasm == 1 else \"not sarcastic\"\n",
        "\n",
        "raw_df_train['intent'] = raw_df_train['sarcastic'].apply(map_sarcasm_to_intent)\n",
        "\n",
        "# Rename the tweet column to utterance\n",
        "raw_df_train.rename(columns={'tweet': 'utterance'}, inplace=True)\n",
        "raw_df_train = raw_df_train[[\"intent\", \"utterance\"]]\n",
        "raw_df_train = shuffle(raw_df_train)\n",
        "raw_df_train.to_csv('train.csv', index=False)\n",
        "print(f\"Train data length: {len(raw_df_train)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBXWzuIgY8wi",
        "outputId": "70f7fb97-eb58-44be-b025-b61aa137d0b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data length: 3468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune_gpt.py \\\n",
        "    --output_dir='/content/transformers/output' \\\n",
        "    --model_type=gpt2-medium \\\n",
        "    --model_name_or_path=gpt2-medium \\\n",
        "    --num_train_epochs=3 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=/content/train.csv \\\n",
        "    --per_gpu_train_batch_size=24 \\\n",
        "    --block_size=10 \\\n",
        "    --gradient_accumulation_steps=1 \\\n",
        "    --line_by_line \\\n",
        "    --overwrite_output_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMQwMlfbY-Lq",
        "outputId": "b4f0b4d1-f2ad-44eb-865c-c47ab0a50766"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-21 03:01:09.052704: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-21 03:01:09.052772: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-21 03:01:09.054623: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-21 03:01:11.453059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "12/21/2023 03:01:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "config.json: 100% 718/718 [00:00<00:00, 3.15MB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 15.8MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 6.61MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 43.7MB/s]\n",
            "model.safetensors: 100% 1.52G/1.52G [00:15<00:00, 95.6MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 766kB/s]\n",
            "12/21/2023 03:01:35 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='/content/train.csv', output_dir='/content/transformers/output', model_type='gpt2-medium', eval_data_file=None, line_by_line=True, should_continue=False, model_name_or_path='gpt2-medium', mlm=False, mlm_probability=0.15, config_name=None, tokenizer_name=None, cache_dir=None, block_size=10, do_train=True, do_eval=False, evaluate_during_training=False, per_gpu_train_batch_size=24, per_gpu_eval_batch_size=4, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=500, save_steps=500, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'))\n",
            "12/21/2023 03:01:35 - INFO - __main__ -   Creating features from dataset file at /content/train.csv\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "12/21/2023 03:01:36 - INFO - __main__ -   ***** Running training *****\n",
            "12/21/2023 03:01:36 - INFO - __main__ -     Num examples = 3828\n",
            "12/21/2023 03:01:36 - INFO - __main__ -     Num Epochs = 3\n",
            "12/21/2023 03:01:36 - INFO - __main__ -     Instantaneous batch size per GPU = 24\n",
            "12/21/2023 03:01:36 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 24\n",
            "12/21/2023 03:01:36 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "12/21/2023 03:01:36 - INFO - __main__ -     Total optimization steps = 480\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/160 [00:01<04:04,  1.54s/it]\u001b[A\n",
            "Iteration:   1% 2/160 [00:01<02:07,  1.24it/s]\u001b[A\n",
            "Iteration:   2% 3/160 [00:02<01:29,  1.75it/s]\u001b[A\n",
            "Iteration:   2% 4/160 [00:02<01:11,  2.18it/s]\u001b[A\n",
            "Iteration:   3% 5/160 [00:02<01:01,  2.51it/s]\u001b[A\n",
            "Iteration:   4% 6/160 [00:02<00:55,  2.75it/s]\u001b[A\n",
            "Iteration:   4% 7/160 [00:03<00:51,  2.94it/s]\u001b[A\n",
            "Iteration:   5% 8/160 [00:03<00:49,  3.09it/s]\u001b[A\n",
            "Iteration:   6% 9/160 [00:03<00:47,  3.17it/s]\u001b[A\n",
            "Iteration:   6% 10/160 [00:04<00:46,  3.25it/s]\u001b[A\n",
            "Iteration:   7% 11/160 [00:04<00:45,  3.30it/s]\u001b[A\n",
            "Iteration:   8% 12/160 [00:04<00:44,  3.34it/s]\u001b[A\n",
            "Iteration:   8% 13/160 [00:05<00:43,  3.35it/s]\u001b[A\n",
            "Iteration:   9% 14/160 [00:05<00:43,  3.37it/s]\u001b[A\n",
            "Iteration:   9% 15/160 [00:05<00:42,  3.39it/s]\u001b[A\n",
            "Iteration:  10% 16/160 [00:05<00:42,  3.41it/s]\u001b[A\n",
            "Iteration:  11% 17/160 [00:06<00:41,  3.43it/s]\u001b[A\n",
            "Iteration:  11% 18/160 [00:06<00:40,  3.50it/s]\u001b[A\n",
            "Iteration:  12% 19/160 [00:06<00:40,  3.47it/s]\u001b[A\n",
            "Iteration:  12% 20/160 [00:07<00:40,  3.45it/s]\u001b[A\n",
            "Iteration:  13% 21/160 [00:07<00:40,  3.45it/s]\u001b[A\n",
            "Iteration:  14% 22/160 [00:07<00:39,  3.45it/s]\u001b[A\n",
            "Iteration:  14% 23/160 [00:07<00:39,  3.49it/s]\u001b[A\n",
            "Iteration:  15% 24/160 [00:08<00:39,  3.45it/s]\u001b[A\n",
            "Iteration:  16% 25/160 [00:08<00:39,  3.45it/s]\u001b[A\n",
            "Iteration:  16% 26/160 [00:08<00:38,  3.46it/s]\u001b[A\n",
            "Iteration:  17% 27/160 [00:09<00:38,  3.46it/s]\u001b[A\n",
            "Iteration:  18% 28/160 [00:09<00:38,  3.44it/s]\u001b[A\n",
            "Iteration:  18% 29/160 [00:09<00:38,  3.44it/s]\u001b[A\n",
            "Iteration:  19% 30/160 [00:09<00:37,  3.44it/s]\u001b[A\n",
            "Iteration:  19% 31/160 [00:10<00:37,  3.45it/s]\u001b[A\n",
            "Iteration:  20% 32/160 [00:10<00:37,  3.45it/s]\u001b[A\n",
            "Iteration:  21% 33/160 [00:10<00:36,  3.46it/s]\u001b[A\n",
            "Iteration:  21% 34/160 [00:11<00:36,  3.47it/s]\u001b[A\n",
            "Iteration:  22% 35/160 [00:11<00:36,  3.46it/s]\u001b[A\n",
            "Iteration:  22% 36/160 [00:11<00:35,  3.45it/s]\u001b[A\n",
            "Iteration:  23% 37/160 [00:11<00:35,  3.44it/s]\u001b[A\n",
            "Iteration:  24% 38/160 [00:12<00:35,  3.45it/s]\u001b[A\n",
            "Iteration:  24% 39/160 [00:12<00:35,  3.45it/s]\u001b[A\n",
            "Iteration:  25% 40/160 [00:12<00:34,  3.46it/s]\u001b[A\n",
            "Iteration:  26% 41/160 [00:13<00:34,  3.46it/s]\u001b[A\n",
            "Iteration:  26% 42/160 [00:13<00:34,  3.45it/s]\u001b[A\n",
            "Iteration:  27% 43/160 [00:13<00:33,  3.46it/s]\u001b[A\n",
            "Iteration:  28% 44/160 [00:14<00:33,  3.46it/s]\u001b[A\n",
            "Iteration:  28% 45/160 [00:14<00:33,  3.46it/s]\u001b[A\n",
            "Iteration:  29% 46/160 [00:14<00:32,  3.46it/s]\u001b[A\n",
            "Iteration:  29% 47/160 [00:14<00:32,  3.47it/s]\u001b[A\n",
            "Iteration:  30% 48/160 [00:15<00:32,  3.46it/s]\u001b[A\n",
            "Iteration:  31% 49/160 [00:15<00:32,  3.45it/s]\u001b[A\n",
            "Iteration:  31% 50/160 [00:15<00:32,  3.43it/s]\u001b[A\n",
            "Iteration:  32% 51/160 [00:16<00:31,  3.46it/s]\u001b[A\n",
            "Iteration:  32% 52/160 [00:16<00:31,  3.42it/s]\u001b[A\n",
            "Iteration:  33% 53/160 [00:16<00:31,  3.41it/s]\u001b[A\n",
            "Iteration:  34% 54/160 [00:16<00:31,  3.40it/s]\u001b[A\n",
            "Iteration:  34% 55/160 [00:17<00:30,  3.40it/s]\u001b[A\n",
            "Iteration:  35% 56/160 [00:17<00:30,  3.38it/s]\u001b[A\n",
            "Iteration:  36% 57/160 [00:17<00:30,  3.38it/s]\u001b[A\n",
            "Iteration:  36% 58/160 [00:18<00:30,  3.38it/s]\u001b[A\n",
            "Iteration:  37% 59/160 [00:18<00:30,  3.36it/s]\u001b[A\n",
            "Iteration:  38% 60/160 [00:18<00:29,  3.39it/s]\u001b[A\n",
            "Iteration:  38% 61/160 [00:18<00:29,  3.39it/s]\u001b[A\n",
            "Iteration:  39% 62/160 [00:19<00:28,  3.39it/s]\u001b[A\n",
            "Iteration:  39% 63/160 [00:19<00:28,  3.40it/s]\u001b[A\n",
            "Iteration:  40% 64/160 [00:19<00:28,  3.42it/s]\u001b[A\n",
            "Iteration:  41% 65/160 [00:20<00:27,  3.43it/s]\u001b[A\n",
            "Iteration:  41% 66/160 [00:20<00:27,  3.42it/s]\u001b[A\n",
            "Iteration:  42% 67/160 [00:20<00:27,  3.44it/s]\u001b[A\n",
            "Iteration:  42% 68/160 [00:21<00:26,  3.44it/s]\u001b[A\n",
            "Iteration:  43% 69/160 [00:21<00:26,  3.42it/s]\u001b[A\n",
            "Iteration:  44% 70/160 [00:21<00:26,  3.42it/s]\u001b[A\n",
            "Iteration:  44% 71/160 [00:21<00:26,  3.42it/s]\u001b[A\n",
            "Iteration:  45% 72/160 [00:22<00:25,  3.41it/s]\u001b[A\n",
            "Iteration:  46% 73/160 [00:22<00:25,  3.41it/s]\u001b[A\n",
            "Iteration:  46% 74/160 [00:22<00:25,  3.41it/s]\u001b[A\n",
            "Iteration:  47% 75/160 [00:23<00:24,  3.42it/s]\u001b[A\n",
            "Iteration:  48% 76/160 [00:23<00:24,  3.41it/s]\u001b[A\n",
            "Iteration:  48% 77/160 [00:23<00:24,  3.42it/s]\u001b[A\n",
            "Iteration:  49% 78/160 [00:23<00:23,  3.43it/s]\u001b[A\n",
            "Iteration:  49% 79/160 [00:24<00:23,  3.42it/s]\u001b[A\n",
            "Iteration:  50% 80/160 [00:24<00:23,  3.43it/s]\u001b[A\n",
            "Iteration:  51% 81/160 [00:24<00:23,  3.42it/s]\u001b[A\n",
            "Iteration:  51% 82/160 [00:25<00:22,  3.42it/s]\u001b[A\n",
            "Iteration:  52% 83/160 [00:25<00:22,  3.43it/s]\u001b[A\n",
            "Iteration:  52% 84/160 [00:25<00:22,  3.43it/s]\u001b[A\n",
            "Iteration:  53% 85/160 [00:26<00:21,  3.43it/s]\u001b[A\n",
            "Iteration:  54% 86/160 [00:26<00:21,  3.42it/s]\u001b[A\n",
            "Iteration:  54% 87/160 [00:26<00:21,  3.42it/s]\u001b[A\n",
            "Iteration:  55% 88/160 [00:26<00:21,  3.43it/s]\u001b[A\n",
            "Iteration:  56% 89/160 [00:27<00:20,  3.43it/s]\u001b[A\n",
            "Iteration:  56% 90/160 [00:27<00:20,  3.43it/s]\u001b[A\n",
            "Iteration:  57% 91/160 [00:27<00:20,  3.43it/s]\u001b[A\n",
            "Iteration:  57% 92/160 [00:28<00:19,  3.43it/s]\u001b[A\n",
            "Iteration:  58% 93/160 [00:28<00:19,  3.41it/s]\u001b[A\n",
            "Iteration:  59% 94/160 [00:28<00:19,  3.37it/s]\u001b[A\n",
            "Iteration:  59% 95/160 [00:28<00:19,  3.37it/s]\u001b[A\n",
            "Iteration:  60% 96/160 [00:29<00:18,  3.37it/s]\u001b[A\n",
            "Iteration:  61% 97/160 [00:29<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  61% 98/160 [00:29<00:18,  3.42it/s]\u001b[A\n",
            "Iteration:  62% 99/160 [00:30<00:18,  3.39it/s]\u001b[A\n",
            "Iteration:  62% 100/160 [00:30<00:17,  3.38it/s]\u001b[A\n",
            "Iteration:  63% 101/160 [00:30<00:17,  3.37it/s]\u001b[A\n",
            "Iteration:  64% 102/160 [00:31<00:17,  3.38it/s]\u001b[A\n",
            "Iteration:  64% 103/160 [00:31<00:16,  3.37it/s]\u001b[A\n",
            "Iteration:  65% 104/160 [00:31<00:16,  3.39it/s]\u001b[A\n",
            "Iteration:  66% 105/160 [00:31<00:16,  3.40it/s]\u001b[A\n",
            "Iteration:  66% 106/160 [00:32<00:15,  3.41it/s]\u001b[A\n",
            "Iteration:  67% 107/160 [00:32<00:15,  3.41it/s]\u001b[A\n",
            "Iteration:  68% 108/160 [00:32<00:15,  3.42it/s]\u001b[A\n",
            "Iteration:  68% 109/160 [00:33<00:14,  3.42it/s]\u001b[A\n",
            "Iteration:  69% 110/160 [00:33<00:14,  3.42it/s]\u001b[A\n",
            "Iteration:  69% 111/160 [00:33<00:14,  3.41it/s]\u001b[A\n",
            "Iteration:  70% 112/160 [00:33<00:14,  3.41it/s]\u001b[A\n",
            "Iteration:  71% 113/160 [00:34<00:13,  3.41it/s]\u001b[A\n",
            "Iteration:  71% 114/160 [00:34<00:13,  3.40it/s]\u001b[A\n",
            "Iteration:  72% 115/160 [00:34<00:13,  3.40it/s]\u001b[A\n",
            "Iteration:  72% 116/160 [00:35<00:12,  3.40it/s]\u001b[A\n",
            "Iteration:  73% 117/160 [00:35<00:12,  3.40it/s]\u001b[A\n",
            "Iteration:  74% 118/160 [00:35<00:12,  3.42it/s]\u001b[A\n",
            "Iteration:  74% 119/160 [00:36<00:12,  3.42it/s]\u001b[A\n",
            "Iteration:  75% 120/160 [00:36<00:11,  3.41it/s]\u001b[A\n",
            "Iteration:  76% 121/160 [00:36<00:11,  3.41it/s]\u001b[A\n",
            "Iteration:  76% 122/160 [00:36<00:11,  3.40it/s]\u001b[A\n",
            "Iteration:  77% 123/160 [00:37<00:10,  3.42it/s]\u001b[A\n",
            "Iteration:  78% 124/160 [00:37<00:10,  3.41it/s]\u001b[A\n",
            "Iteration:  78% 125/160 [00:37<00:10,  3.41it/s]\u001b[A\n",
            "Iteration:  79% 126/160 [00:38<00:09,  3.40it/s]\u001b[A\n",
            "Iteration:  79% 127/160 [00:38<00:09,  3.41it/s]\u001b[A\n",
            "Iteration:  80% 128/160 [00:38<00:09,  3.41it/s]\u001b[A\n",
            "Iteration:  81% 129/160 [00:38<00:09,  3.40it/s]\u001b[A\n",
            "Iteration:  81% 130/160 [00:39<00:08,  3.41it/s]\u001b[A\n",
            "Iteration:  82% 131/160 [00:39<00:08,  3.41it/s]\u001b[A\n",
            "Iteration:  82% 132/160 [00:39<00:08,  3.49it/s]\u001b[A\n",
            "Iteration:  83% 133/160 [00:40<00:07,  3.41it/s]\u001b[A\n",
            "Iteration:  84% 134/160 [00:40<00:07,  3.39it/s]\u001b[A\n",
            "Iteration:  84% 135/160 [00:40<00:07,  3.39it/s]\u001b[A\n",
            "Iteration:  85% 136/160 [00:40<00:07,  3.39it/s]\u001b[A\n",
            "Iteration:  86% 137/160 [00:41<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  86% 138/160 [00:41<00:06,  3.37it/s]\u001b[A\n",
            "Iteration:  87% 139/160 [00:41<00:06,  3.36it/s]\u001b[A\n",
            "Iteration:  88% 140/160 [00:42<00:05,  3.36it/s]\u001b[A\n",
            "Iteration:  88% 141/160 [00:42<00:05,  3.36it/s]\u001b[A\n",
            "Iteration:  89% 142/160 [00:42<00:05,  3.36it/s]\u001b[A\n",
            "Iteration:  89% 143/160 [00:43<00:05,  3.39it/s]\u001b[A\n",
            "Iteration:  90% 144/160 [00:43<00:04,  3.35it/s]\u001b[A\n",
            "Iteration:  91% 145/160 [00:43<00:04,  3.33it/s]\u001b[A\n",
            "Iteration:  91% 146/160 [00:43<00:04,  3.32it/s]\u001b[A\n",
            "Iteration:  92% 147/160 [00:44<00:03,  3.32it/s]\u001b[A\n",
            "Iteration:  92% 148/160 [00:44<00:03,  3.35it/s]\u001b[A\n",
            "Iteration:  93% 149/160 [00:44<00:03,  3.35it/s]\u001b[A\n",
            "Iteration:  94% 150/160 [00:45<00:02,  3.36it/s]\u001b[A\n",
            "Iteration:  94% 151/160 [00:45<00:02,  3.36it/s]\u001b[A\n",
            "Iteration:  95% 152/160 [00:45<00:02,  3.37it/s]\u001b[A\n",
            "Iteration:  96% 153/160 [00:46<00:02,  3.37it/s]\u001b[A\n",
            "Iteration:  96% 154/160 [00:46<00:01,  3.38it/s]\u001b[A\n",
            "Iteration:  97% 155/160 [00:46<00:01,  3.38it/s]\u001b[A\n",
            "Iteration:  98% 156/160 [00:46<00:01,  3.37it/s]\u001b[A\n",
            "Iteration:  98% 157/160 [00:47<00:00,  3.38it/s]\u001b[A\n",
            "Iteration:  99% 158/160 [00:47<00:00,  3.37it/s]\u001b[A\n",
            "Iteration:  99% 159/160 [00:47<00:00,  3.35it/s]\u001b[A\n",
            "Iteration: 100% 160/160 [00:48<00:00,  3.33it/s]\n",
            "Epoch:  33% 1/3 [00:48<01:36, 48.08s/it]\n",
            "Iteration:   0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/160 [00:00<00:50,  3.12it/s]\u001b[A\n",
            "Iteration:   1% 2/160 [00:00<00:51,  3.08it/s]\u001b[A\n",
            "Iteration:   2% 3/160 [00:00<00:49,  3.20it/s]\u001b[A\n",
            "Iteration:   2% 4/160 [00:01<00:49,  3.14it/s]\u001b[A\n",
            "Iteration:   3% 5/160 [00:01<00:48,  3.17it/s]\u001b[A\n",
            "Iteration:   4% 6/160 [00:01<00:47,  3.24it/s]\u001b[A\n",
            "Iteration:   4% 7/160 [00:02<00:47,  3.25it/s]\u001b[A\n",
            "Iteration:   5% 8/160 [00:02<00:49,  3.06it/s]\u001b[A\n",
            "Iteration:   6% 9/160 [00:02<00:47,  3.15it/s]\u001b[A\n",
            "Iteration:   6% 10/160 [00:03<00:46,  3.22it/s]\u001b[A\n",
            "Iteration:   7% 11/160 [00:03<00:45,  3.25it/s]\u001b[A\n",
            "Iteration:   8% 12/160 [00:03<00:45,  3.28it/s]\u001b[A\n",
            "Iteration:   8% 13/160 [00:04<00:44,  3.31it/s]\u001b[A\n",
            "Iteration:   9% 14/160 [00:04<00:44,  3.31it/s]\u001b[A\n",
            "Iteration:   9% 15/160 [00:04<00:43,  3.34it/s]\u001b[A\n",
            "Iteration:  10% 16/160 [00:04<00:43,  3.34it/s]\u001b[A\n",
            "Iteration:  11% 17/160 [00:05<00:42,  3.35it/s]\u001b[A\n",
            "Iteration:  11% 18/160 [00:05<00:42,  3.35it/s]\u001b[A\n",
            "Iteration:  12% 19/160 [00:05<00:41,  3.36it/s]\u001b[A\n",
            "Iteration:  12% 20/160 [00:06<00:41,  3.37it/s]\u001b[A\n",
            "Iteration:  13% 21/160 [00:06<00:41,  3.34it/s]\u001b[A\n",
            "Iteration:  14% 22/160 [00:06<00:41,  3.32it/s]\u001b[A\n",
            "Iteration:  14% 23/160 [00:07<00:41,  3.28it/s]\u001b[A\n",
            "Iteration:  15% 24/160 [00:07<00:40,  3.33it/s]\u001b[A\n",
            "Iteration:  16% 25/160 [00:07<00:40,  3.32it/s]\u001b[A\n",
            "Iteration:  16% 26/160 [00:07<00:40,  3.30it/s]\u001b[A\n",
            "Iteration:  17% 27/160 [00:08<00:40,  3.29it/s]\u001b[A\n",
            "Iteration:  18% 28/160 [00:08<00:40,  3.29it/s]\u001b[A\n",
            "Iteration:  18% 29/160 [00:08<00:39,  3.28it/s]\u001b[A\n",
            "Iteration:  19% 30/160 [00:09<00:39,  3.29it/s]\u001b[A\n",
            "Iteration:  19% 31/160 [00:09<00:39,  3.30it/s]\u001b[A\n",
            "Iteration:  20% 32/160 [00:09<00:38,  3.32it/s]\u001b[A\n",
            "Iteration:  21% 33/160 [00:10<00:38,  3.34it/s]\u001b[A\n",
            "Iteration:  21% 34/160 [00:10<00:37,  3.33it/s]\u001b[A\n",
            "Iteration:  22% 35/160 [00:10<00:37,  3.33it/s]\u001b[A\n",
            "Iteration:  22% 36/160 [00:10<00:37,  3.33it/s]\u001b[A\n",
            "Iteration:  23% 37/160 [00:11<00:36,  3.34it/s]\u001b[A\n",
            "Iteration:  24% 38/160 [00:11<00:36,  3.34it/s]\u001b[A\n",
            "Iteration:  24% 39/160 [00:11<00:36,  3.33it/s]\u001b[A\n",
            "Iteration:  25% 40/160 [00:12<00:35,  3.34it/s]\u001b[A\n",
            "Iteration:  26% 41/160 [00:12<00:35,  3.34it/s]\u001b[A\n",
            "Iteration:  26% 42/160 [00:12<00:35,  3.34it/s]\u001b[A\n",
            "Iteration:  27% 43/160 [00:13<00:35,  3.33it/s]\u001b[A\n",
            "Iteration:  28% 44/160 [00:13<00:34,  3.32it/s]\u001b[A\n",
            "Iteration:  28% 45/160 [00:13<00:34,  3.33it/s]\u001b[A\n",
            "Iteration:  29% 46/160 [00:13<00:34,  3.32it/s]\u001b[A\n",
            "Iteration:  29% 47/160 [00:14<00:33,  3.33it/s]\u001b[A\n",
            "Iteration:  30% 48/160 [00:14<00:33,  3.32it/s]\u001b[A\n",
            "Iteration:  31% 49/160 [00:14<00:33,  3.32it/s]\u001b[A\n",
            "Iteration:  31% 50/160 [00:15<00:33,  3.32it/s]\u001b[A\n",
            "Iteration:  32% 51/160 [00:15<00:32,  3.31it/s]\u001b[A\n",
            "Iteration:  32% 52/160 [00:15<00:32,  3.33it/s]\u001b[A\n",
            "Iteration:  33% 53/160 [00:16<00:32,  3.33it/s]\u001b[A\n",
            "Iteration:  34% 54/160 [00:16<00:31,  3.32it/s]\u001b[A\n",
            "Iteration:  34% 55/160 [00:16<00:31,  3.32it/s]\u001b[A\n",
            "Iteration:  35% 56/160 [00:16<00:31,  3.32it/s]\u001b[A\n",
            "Iteration:  36% 57/160 [00:17<00:30,  3.32it/s]\u001b[A\n",
            "Iteration:  36% 58/160 [00:17<00:30,  3.32it/s]\u001b[A\n",
            "Iteration:  37% 59/160 [00:17<00:30,  3.32it/s]\u001b[A\n",
            "Iteration:  38% 60/160 [00:18<00:30,  3.32it/s]\u001b[A\n",
            "Iteration:  38% 61/160 [00:18<00:29,  3.32it/s]\u001b[A\n",
            "Iteration:  39% 62/160 [00:18<00:29,  3.33it/s]\u001b[A\n",
            "Iteration:  39% 63/160 [00:19<00:29,  3.33it/s]\u001b[A\n",
            "Iteration:  40% 64/160 [00:19<00:29,  3.31it/s]\u001b[A\n",
            "Iteration:  41% 65/160 [00:19<00:28,  3.29it/s]\u001b[A\n",
            "Iteration:  41% 66/160 [00:20<00:28,  3.27it/s]\u001b[A\n",
            "Iteration:  42% 67/160 [00:20<00:28,  3.27it/s]\u001b[A\n",
            "Iteration:  42% 68/160 [00:20<00:28,  3.27it/s]\u001b[A\n",
            "Iteration:  43% 69/160 [00:20<00:27,  3.27it/s]\u001b[A\n",
            "Iteration:  44% 70/160 [00:21<00:27,  3.28it/s]\u001b[A\n",
            "Iteration:  44% 71/160 [00:21<00:27,  3.27it/s]\u001b[A\n",
            "Iteration:  45% 72/160 [00:21<00:26,  3.26it/s]\u001b[A\n",
            "Iteration:  46% 73/160 [00:22<00:26,  3.27it/s]\u001b[A\n",
            "Iteration:  46% 74/160 [00:22<00:26,  3.29it/s]\u001b[A\n",
            "Iteration:  47% 75/160 [00:22<00:25,  3.30it/s]\u001b[A\n",
            "Iteration:  48% 76/160 [00:23<00:25,  3.30it/s]\u001b[A\n",
            "Iteration:  48% 77/160 [00:23<00:25,  3.30it/s]\u001b[A\n",
            "Iteration:  49% 78/160 [00:23<00:24,  3.30it/s]\u001b[A\n",
            "Iteration:  49% 79/160 [00:23<00:24,  3.30it/s]\u001b[A\n",
            "Iteration:  50% 80/160 [00:24<00:24,  3.31it/s]\u001b[A\n",
            "Iteration:  51% 81/160 [00:24<00:23,  3.31it/s]\u001b[A\n",
            "Iteration:  51% 82/160 [00:24<00:23,  3.31it/s]\u001b[A\n",
            "Iteration:  52% 83/160 [00:25<00:23,  3.31it/s]\u001b[A\n",
            "Iteration:  52% 84/160 [00:25<00:23,  3.30it/s]\u001b[A\n",
            "Iteration:  53% 85/160 [00:25<00:22,  3.31it/s]\u001b[A\n",
            "Iteration:  54% 86/160 [00:26<00:22,  3.31it/s]\u001b[A\n",
            "Iteration:  54% 87/160 [00:26<00:22,  3.31it/s]\u001b[A\n",
            "Iteration:  55% 88/160 [00:26<00:21,  3.32it/s]\u001b[A\n",
            "Iteration:  56% 89/160 [00:26<00:21,  3.31it/s]\u001b[A\n",
            "Iteration:  56% 90/160 [00:27<00:21,  3.30it/s]\u001b[A\n",
            "Iteration:  57% 91/160 [00:27<00:20,  3.30it/s]\u001b[A\n",
            "Iteration:  57% 92/160 [00:27<00:20,  3.30it/s]\u001b[A\n",
            "Iteration:  58% 93/160 [00:28<00:20,  3.30it/s]\u001b[A\n",
            "Iteration:  59% 94/160 [00:28<00:20,  3.29it/s]\u001b[A\n",
            "Iteration:  59% 95/160 [00:28<00:19,  3.29it/s]\u001b[A\n",
            "Iteration:  60% 96/160 [00:29<00:19,  3.30it/s]\u001b[A\n",
            "Iteration:  61% 97/160 [00:29<00:19,  3.29it/s]\u001b[A\n",
            "Iteration:  61% 98/160 [00:29<00:18,  3.30it/s]\u001b[A\n",
            "Iteration:  62% 99/160 [00:30<00:18,  3.28it/s]\u001b[A\n",
            "Iteration:  62% 100/160 [00:30<00:18,  3.29it/s]\u001b[A\n",
            "Iteration:  63% 101/160 [00:30<00:17,  3.29it/s]\u001b[A\n",
            "Iteration:  64% 102/160 [00:30<00:17,  3.28it/s]\u001b[A\n",
            "Iteration:  64% 103/160 [00:31<00:17,  3.28it/s]\u001b[A\n",
            "Iteration:  65% 104/160 [00:31<00:17,  3.29it/s]\u001b[A\n",
            "Iteration:  66% 105/160 [00:31<00:16,  3.29it/s]\u001b[A\n",
            "Iteration:  66% 106/160 [00:32<00:16,  3.28it/s]\u001b[A\n",
            "Iteration:  67% 107/160 [00:32<00:16,  3.27it/s]\u001b[A\n",
            "Iteration:  68% 108/160 [00:32<00:15,  3.27it/s]\u001b[A\n",
            "Iteration:  68% 109/160 [00:33<00:15,  3.24it/s]\u001b[A\n",
            "Iteration:  69% 110/160 [00:33<00:15,  3.24it/s]\u001b[A\n",
            "Iteration:  69% 111/160 [00:33<00:15,  3.24it/s]\u001b[A\n",
            "Iteration:  70% 112/160 [00:34<00:14,  3.22it/s]\u001b[A\n",
            "Iteration:  71% 113/160 [00:34<00:14,  3.22it/s]\u001b[A\n",
            "Iteration:  71% 114/160 [00:34<00:14,  3.22it/s]\u001b[A\n",
            "Iteration:  72% 115/160 [00:34<00:13,  3.23it/s]\u001b[A\n",
            "Iteration:  72% 116/160 [00:35<00:13,  3.25it/s]\u001b[A\n",
            "Iteration:  73% 117/160 [00:35<00:13,  3.26it/s]\u001b[A\n",
            "Iteration:  74% 118/160 [00:35<00:12,  3.27it/s]\u001b[A\n",
            "Iteration:  74% 119/160 [00:36<00:12,  3.28it/s]\u001b[A\n",
            "Iteration:  75% 120/160 [00:36<00:12,  3.28it/s]\u001b[A\n",
            "Iteration:  76% 121/160 [00:36<00:11,  3.27it/s]\u001b[A\n",
            "Iteration:  76% 122/160 [00:37<00:11,  3.27it/s]\u001b[A\n",
            "Iteration:  77% 123/160 [00:37<00:11,  3.27it/s]\u001b[A\n",
            "Iteration:  78% 124/160 [00:37<00:10,  3.28it/s]\u001b[A\n",
            "Iteration:  78% 125/160 [00:37<00:10,  3.28it/s]\u001b[A\n",
            "Iteration:  79% 126/160 [00:38<00:10,  3.28it/s]\u001b[A\n",
            "Iteration:  79% 127/160 [00:38<00:10,  3.28it/s]\u001b[A\n",
            "Iteration:  80% 128/160 [00:38<00:09,  3.28it/s]\u001b[A\n",
            "Iteration:  81% 129/160 [00:39<00:09,  3.29it/s]\u001b[A\n",
            "Iteration:  81% 130/160 [00:39<00:09,  3.29it/s]\u001b[A\n",
            "Iteration:  82% 131/160 [00:39<00:08,  3.29it/s]\u001b[A\n",
            "Iteration:  82% 132/160 [00:40<00:08,  3.28it/s]\u001b[A\n",
            "Iteration:  83% 133/160 [00:40<00:08,  3.29it/s]\u001b[A\n",
            "Iteration:  84% 134/160 [00:40<00:07,  3.28it/s]\u001b[A\n",
            "Iteration:  84% 135/160 [00:41<00:07,  3.29it/s]\u001b[A\n",
            "Iteration:  85% 136/160 [00:41<00:07,  3.28it/s]\u001b[A\n",
            "Iteration:  86% 137/160 [00:41<00:06,  3.29it/s]\u001b[A\n",
            "Iteration:  86% 138/160 [00:41<00:06,  3.28it/s]\u001b[A\n",
            "Iteration:  87% 139/160 [00:42<00:06,  3.28it/s]\u001b[A\n",
            "Iteration:  88% 140/160 [00:42<00:06,  3.28it/s]\u001b[A\n",
            "Iteration:  88% 141/160 [00:42<00:05,  3.28it/s]\u001b[A\n",
            "Iteration:  89% 142/160 [00:43<00:05,  3.29it/s]\u001b[A\n",
            "Iteration:  89% 143/160 [00:43<00:05,  3.28it/s]\u001b[A\n",
            "Iteration:  90% 144/160 [00:43<00:04,  3.28it/s]\u001b[A\n",
            "Iteration:  91% 145/160 [00:44<00:04,  3.28it/s]\u001b[A\n",
            "Iteration:  91% 146/160 [00:44<00:04,  3.29it/s]\u001b[A\n",
            "Iteration:  92% 147/160 [00:44<00:03,  3.28it/s]\u001b[A\n",
            "Iteration:  92% 148/160 [00:44<00:03,  3.27it/s]\u001b[A\n",
            "Iteration:  93% 149/160 [00:45<00:03,  3.26it/s]\u001b[A\n",
            "Iteration:  94% 150/160 [00:45<00:03,  3.24it/s]\u001b[A\n",
            "Iteration:  94% 151/160 [00:45<00:02,  3.24it/s]\u001b[A\n",
            "Iteration:  95% 152/160 [00:46<00:02,  3.23it/s]\u001b[A\n",
            "Iteration:  96% 153/160 [00:46<00:02,  3.23it/s]\u001b[A\n",
            "Iteration:  96% 154/160 [00:46<00:01,  3.22it/s]\u001b[A\n",
            "Iteration:  97% 155/160 [00:47<00:01,  3.23it/s]\u001b[A\n",
            "Iteration:  98% 156/160 [00:47<00:01,  3.22it/s]\u001b[A\n",
            "Iteration:  98% 157/160 [00:47<00:00,  3.23it/s]\u001b[A\n",
            "Iteration:  99% 158/160 [00:48<00:00,  3.23it/s]\u001b[A\n",
            "Iteration:  99% 159/160 [00:48<00:00,  3.24it/s]\u001b[A\n",
            "Iteration: 100% 160/160 [00:48<00:00,  3.29it/s]\n",
            "Epoch:  67% 2/3 [01:36<00:48, 48.40s/it]\n",
            "Iteration:   0% 0/160 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/160 [00:00<00:48,  3.26it/s]\u001b[A\n",
            "Iteration:   1% 2/160 [00:00<00:48,  3.25it/s]\u001b[A\n",
            "Iteration:   2% 3/160 [00:00<00:48,  3.25it/s]\u001b[A\n",
            "Iteration:   2% 4/160 [00:01<00:47,  3.27it/s]\u001b[A\n",
            "Iteration:   3% 5/160 [00:01<00:47,  3.28it/s]\u001b[A\n",
            "Iteration:   4% 6/160 [00:01<00:46,  3.28it/s]\u001b[A\n",
            "Iteration:   4% 7/160 [00:02<00:46,  3.29it/s]\u001b[A\n",
            "Iteration:   5% 8/160 [00:02<00:46,  3.29it/s]\u001b[A\n",
            "Iteration:   6% 9/160 [00:02<00:45,  3.29it/s]\u001b[A\n",
            "Iteration:   6% 10/160 [00:03<00:45,  3.28it/s]\u001b[A\n",
            "Iteration:   7% 11/160 [00:03<00:45,  3.28it/s]\u001b[A\n",
            "Iteration:   8% 12/160 [00:03<00:45,  3.28it/s]\u001b[A\n",
            "Iteration:   8% 13/160 [00:03<00:44,  3.28it/s]\u001b[A\n",
            "Iteration:   9% 14/160 [00:04<00:44,  3.29it/s]\u001b[A\n",
            "Iteration:   9% 15/160 [00:04<00:44,  3.29it/s]\u001b[A\n",
            "Iteration:  10% 16/160 [00:04<00:43,  3.29it/s]\u001b[A\n",
            "Iteration:  11% 17/160 [00:05<00:43,  3.29it/s]\u001b[A\n",
            "Iteration:  11% 18/160 [00:05<00:43,  3.29it/s]\u001b[A\n",
            "Iteration:  12% 19/160 [00:05<00:42,  3.29it/s]\u001b[A\n",
            "Iteration:  12% 20/160 [00:06<00:42,  3.29it/s]\u001b[A\n",
            "Iteration:  13% 21/160 [00:06<00:42,  3.30it/s]\u001b[A\n",
            "Iteration:  14% 22/160 [00:06<00:41,  3.29it/s]\u001b[A\n",
            "Iteration:  14% 23/160 [00:07<00:41,  3.29it/s]\u001b[A\n",
            "Iteration:  15% 24/160 [00:07<00:41,  3.30it/s]\u001b[A\n",
            "Iteration:  16% 25/160 [00:07<00:40,  3.31it/s]\u001b[A\n",
            "Iteration:  16% 26/160 [00:07<00:40,  3.32it/s]\u001b[A\n",
            "Iteration:  17% 27/160 [00:08<00:40,  3.32it/s]\u001b[A\n",
            "Iteration:  18% 28/160 [00:08<00:39,  3.31it/s]\u001b[A\n",
            "Iteration:  18% 29/160 [00:08<00:39,  3.31it/s]\u001b[A\n",
            "Iteration:  19% 30/160 [00:09<00:39,  3.31it/s]\u001b[A\n",
            "Iteration:  19% 31/160 [00:09<00:39,  3.27it/s]\u001b[A\n",
            "Iteration:  20% 32/160 [00:09<00:39,  3.27it/s]\u001b[A\n",
            "Iteration:  21% 33/160 [00:10<00:38,  3.27it/s]\u001b[A\n",
            "Iteration:  21% 34/160 [00:10<00:38,  3.27it/s]\u001b[A\n",
            "Iteration:  22% 35/160 [00:10<00:38,  3.27it/s]\u001b[A\n",
            "Iteration:  22% 36/160 [00:10<00:37,  3.27it/s]\u001b[A\n",
            "Iteration:  23% 37/160 [00:11<00:37,  3.25it/s]\u001b[A\n",
            "Iteration:  24% 38/160 [00:11<00:37,  3.26it/s]\u001b[A\n",
            "Iteration:  24% 39/160 [00:11<00:37,  3.26it/s]\u001b[A\n",
            "Iteration:  25% 40/160 [00:12<00:36,  3.25it/s]\u001b[A\n",
            "Iteration:  26% 41/160 [00:12<00:36,  3.26it/s]\u001b[A\n",
            "Iteration:  26% 42/160 [00:12<00:36,  3.27it/s]\u001b[A\n",
            "Iteration:  27% 43/160 [00:13<00:35,  3.28it/s]\u001b[A\n",
            "Iteration:  28% 44/160 [00:13<00:35,  3.28it/s]\u001b[A\n",
            "Iteration:  28% 45/160 [00:13<00:35,  3.28it/s]\u001b[A\n",
            "Iteration:  29% 46/160 [00:14<00:34,  3.28it/s]\u001b[A\n",
            "Iteration:  29% 47/160 [00:14<00:34,  3.28it/s]\u001b[A\n",
            "Iteration:  30% 48/160 [00:14<00:34,  3.27it/s]\u001b[A\n",
            "Iteration:  31% 49/160 [00:14<00:33,  3.28it/s]\u001b[A\n",
            "Iteration:  31% 50/160 [00:15<00:33,  3.29it/s]\u001b[A\n",
            "Iteration:  32% 51/160 [00:15<00:33,  3.29it/s]\u001b[A\n",
            "Iteration:  32% 52/160 [00:15<00:32,  3.30it/s]\u001b[A\n",
            "Iteration:  33% 53/160 [00:16<00:32,  3.31it/s]\u001b[A\n",
            "Iteration:  34% 54/160 [00:16<00:32,  3.31it/s]\u001b[A\n",
            "Iteration:  34% 55/160 [00:16<00:31,  3.31it/s]\u001b[A\n",
            "Iteration:  35% 56/160 [00:17<00:31,  3.31it/s]\u001b[A\n",
            "Iteration:  36% 57/160 [00:17<00:31,  3.30it/s]\u001b[A\n",
            "Iteration:  36% 58/160 [00:17<00:30,  3.30it/s]\u001b[A\n",
            "Iteration:  37% 59/160 [00:17<00:30,  3.31it/s]\u001b[A\n",
            "Iteration:  38% 60/160 [00:18<00:30,  3.31it/s]\u001b[A\n",
            "Iteration:  38% 61/160 [00:18<00:29,  3.32it/s]\u001b[A\n",
            "Iteration:  39% 62/160 [00:18<00:29,  3.32it/s]\u001b[A\n",
            "Iteration:  39% 63/160 [00:19<00:29,  3.31it/s]\u001b[A\n",
            "Iteration:  40% 64/160 [00:19<00:28,  3.32it/s]\u001b[A\n",
            "Iteration:  41% 65/160 [00:19<00:28,  3.32it/s]\u001b[A\n",
            "Iteration:  41% 66/160 [00:20<00:28,  3.33it/s]\u001b[A\n",
            "Iteration:  42% 67/160 [00:20<00:27,  3.33it/s]\u001b[A\n",
            "Iteration:  42% 68/160 [00:20<00:27,  3.32it/s]\u001b[A\n",
            "Iteration:  43% 69/160 [00:20<00:27,  3.32it/s]\u001b[A\n",
            "Iteration:  44% 70/160 [00:21<00:27,  3.32it/s]\u001b[A\n",
            "Iteration:  44% 71/160 [00:21<00:26,  3.32it/s]\u001b[A\n",
            "Iteration:  45% 72/160 [00:21<00:26,  3.33it/s]\u001b[A\n",
            "Iteration:  46% 73/160 [00:22<00:26,  3.32it/s]\u001b[A\n",
            "Iteration:  46% 74/160 [00:22<00:26,  3.30it/s]\u001b[A\n",
            "Iteration:  47% 75/160 [00:22<00:25,  3.30it/s]\u001b[A\n",
            "Iteration:  48% 76/160 [00:23<00:25,  3.30it/s]\u001b[A\n",
            "Iteration:  48% 77/160 [00:23<00:25,  3.30it/s]\u001b[A\n",
            "Iteration:  49% 78/160 [00:23<00:24,  3.30it/s]\u001b[A\n",
            "Iteration:  49% 79/160 [00:23<00:24,  3.30it/s]\u001b[A\n",
            "Iteration:  50% 80/160 [00:24<00:24,  3.29it/s]\u001b[A\n",
            "Iteration:  51% 81/160 [00:24<00:24,  3.27it/s]\u001b[A\n",
            "Iteration:  51% 82/160 [00:24<00:23,  3.27it/s]\u001b[A\n",
            "Iteration:  52% 83/160 [00:25<00:23,  3.29it/s]\u001b[A\n",
            "Iteration:  52% 84/160 [00:25<00:23,  3.30it/s]\u001b[A\n",
            "Iteration:  53% 85/160 [00:25<00:22,  3.31it/s]\u001b[A\n",
            "Iteration:  54% 86/160 [00:26<00:22,  3.32it/s]\u001b[A\n",
            "Iteration:  54% 87/160 [00:26<00:21,  3.32it/s]\u001b[A\n",
            "Iteration:  55% 88/160 [00:26<00:21,  3.32it/s]\u001b[A\n",
            "Iteration:  56% 89/160 [00:27<00:21,  3.31it/s]\u001b[A\n",
            "Iteration:  56% 90/160 [00:27<00:21,  3.31it/s]\u001b[A\n",
            "Iteration:  57% 91/160 [00:27<00:20,  3.32it/s]\u001b[A\n",
            "Iteration:  57% 92/160 [00:27<00:20,  3.32it/s]\u001b[A\n",
            "Iteration:  58% 93/160 [00:28<00:20,  3.33it/s]\u001b[A\n",
            "Iteration:  59% 94/160 [00:28<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  59% 95/160 [00:28<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  60% 96/160 [00:29<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  61% 97/160 [00:29<00:18,  3.34it/s]\u001b[A\n",
            "Iteration:  61% 98/160 [00:29<00:18,  3.34it/s]\u001b[A\n",
            "Iteration:  62% 99/160 [00:30<00:18,  3.33it/s]\u001b[A\n",
            "Iteration:  62% 100/160 [00:30<00:18,  3.32it/s]\u001b[A\n",
            "Iteration:  63% 101/160 [00:30<00:17,  3.31it/s]\u001b[A\n",
            "Iteration:  64% 102/160 [00:30<00:17,  3.29it/s]\u001b[A\n",
            "Iteration:  64% 103/160 [00:31<00:17,  3.29it/s]\u001b[A\n",
            "Iteration:  65% 104/160 [00:31<00:17,  3.29it/s]\u001b[A\n",
            "Iteration:  66% 105/160 [00:31<00:16,  3.28it/s]\u001b[A\n",
            "Iteration:  66% 106/160 [00:32<00:16,  3.28it/s]\u001b[A\n",
            "Iteration:  67% 107/160 [00:32<00:16,  3.28it/s]\u001b[A\n",
            "Iteration:  68% 108/160 [00:32<00:15,  3.29it/s]\u001b[A\n",
            "Iteration:  68% 109/160 [00:33<00:15,  3.29it/s]\u001b[A\n",
            "Iteration:  69% 110/160 [00:33<00:15,  3.27it/s]\u001b[A\n",
            "Iteration:  69% 111/160 [00:33<00:14,  3.27it/s]\u001b[A\n",
            "Iteration:  70% 112/160 [00:33<00:14,  3.25it/s]\u001b[A\n",
            "Iteration:  71% 113/160 [00:34<00:14,  3.26it/s]\u001b[A\n",
            "Iteration:  71% 114/160 [00:34<00:14,  3.27it/s]\u001b[A\n",
            "Iteration:  72% 115/160 [00:34<00:13,  3.29it/s]\u001b[A\n",
            "Iteration:  72% 116/160 [00:35<00:13,  3.30it/s]\u001b[A\n",
            "Iteration:  73% 117/160 [00:35<00:13,  3.30it/s]\u001b[A\n",
            "Iteration:  74% 118/160 [00:35<00:12,  3.30it/s]\u001b[A\n",
            "Iteration:  74% 119/160 [00:36<00:12,  3.29it/s]\u001b[A\n",
            "Iteration:  75% 120/160 [00:36<00:12,  3.29it/s]\u001b[A\n",
            "Iteration:  76% 121/160 [00:36<00:11,  3.29it/s]\u001b[A\n",
            "Iteration:  76% 122/160 [00:37<00:11,  3.28it/s]\u001b[A\n",
            "Iteration:  77% 123/160 [00:37<00:11,  3.29it/s]\u001b[A\n",
            "Iteration:  78% 124/160 [00:37<00:10,  3.28it/s]\u001b[A\n",
            "Iteration:  78% 125/160 [00:37<00:10,  3.28it/s]\u001b[A\n",
            "Iteration:  79% 126/160 [00:38<00:10,  3.30it/s]\u001b[A\n",
            "Iteration:  79% 127/160 [00:38<00:09,  3.31it/s]\u001b[A\n",
            "Iteration:  80% 128/160 [00:38<00:09,  3.30it/s]\u001b[A\n",
            "Iteration:  81% 129/160 [00:39<00:09,  3.31it/s]\u001b[A\n",
            "Iteration:  81% 130/160 [00:39<00:09,  3.31it/s]\u001b[A\n",
            "Iteration:  82% 131/160 [00:39<00:08,  3.32it/s]\u001b[A\n",
            "Iteration:  82% 132/160 [00:40<00:08,  3.30it/s]\u001b[A\n",
            "Iteration:  83% 133/160 [00:40<00:08,  3.31it/s]\u001b[A\n",
            "Iteration:  84% 134/160 [00:40<00:07,  3.31it/s]\u001b[A\n",
            "Iteration:  84% 135/160 [00:40<00:07,  3.31it/s]\u001b[A\n",
            "Iteration:  85% 136/160 [00:41<00:07,  3.31it/s]\u001b[A\n",
            "Iteration:  86% 137/160 [00:41<00:06,  3.31it/s]\u001b[A\n",
            "Iteration:  86% 138/160 [00:41<00:06,  3.31it/s]\u001b[A\n",
            "Iteration:  87% 139/160 [00:42<00:06,  3.32it/s]\u001b[A\n",
            "Iteration:  88% 140/160 [00:42<00:06,  3.32it/s]\u001b[A\n",
            "Iteration:  88% 141/160 [00:42<00:05,  3.32it/s]\u001b[A\n",
            "Iteration:  89% 142/160 [00:43<00:05,  3.31it/s]\u001b[A\n",
            "Iteration:  89% 143/160 [00:43<00:05,  3.31it/s]\u001b[A\n",
            "Iteration:  90% 144/160 [00:43<00:04,  3.32it/s]\u001b[A\n",
            "Iteration:  91% 145/160 [00:43<00:04,  3.31it/s]\u001b[A\n",
            "Iteration:  91% 146/160 [00:44<00:04,  3.31it/s]\u001b[A\n",
            "Iteration:  92% 147/160 [00:44<00:03,  3.32it/s]\u001b[A\n",
            "Iteration:  92% 148/160 [00:44<00:03,  3.32it/s]\u001b[A\n",
            "Iteration:  93% 149/160 [00:45<00:03,  3.32it/s]\u001b[A\n",
            "Iteration:  94% 150/160 [00:45<00:03,  3.32it/s]\u001b[A\n",
            "Iteration:  94% 151/160 [00:45<00:02,  3.32it/s]\u001b[A\n",
            "Iteration:  95% 152/160 [00:46<00:02,  3.31it/s]\u001b[A\n",
            "Iteration:  96% 153/160 [00:46<00:02,  3.31it/s]\u001b[A\n",
            "Iteration:  96% 154/160 [00:46<00:01,  3.31it/s]\u001b[A\n",
            "Iteration:  97% 155/160 [00:46<00:01,  3.32it/s]\u001b[A\n",
            "Iteration:  98% 156/160 [00:47<00:01,  3.32it/s]\u001b[A\n",
            "Iteration:  98% 157/160 [00:47<00:00,  3.32it/s]\u001b[A\n",
            "Iteration:  99% 158/160 [00:47<00:00,  3.32it/s]\u001b[A\n",
            "Iteration:  99% 159/160 [00:48<00:00,  3.30it/s]\u001b[A\n",
            "Iteration: 100% 160/160 [00:48<00:00,  3.30it/s]\n",
            "Epoch: 100% 3/3 [02:25<00:00, 48.38s/it]\n",
            "12/21/2023 03:04:01 - INFO - __main__ -    global_step = 480, average loss = 3.119303550819556\n",
            "12/21/2023 03:04:01 - INFO - __main__ -   Saving model checkpoint to /content/transformers/output\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1509: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and manually test model"
      ],
      "metadata": {
        "id": "ikpO2EwGZzCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained('/content/transformers/output/', pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxzF09s5Z0qu",
        "outputId": "e4a93c2f-9192-4891-bc08-a849b1b613e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Steadforce code settings produced this:\n",
        "# Output:\n",
        "# ----------------------------------------------------------------------------------------------------\n",
        "# 0: sarcastic,@Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Sant\n",
        "# 1: sarcastic,@Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Sant\n",
        "# 2: sarcastic,@Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Sant\n",
        "# 3: sarcastic,@Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Sant\n",
        "# 4: sarcastic,@Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Sant\n",
        "# 5: sarcastic,@Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Santandave1 @Sant"
      ],
      "metadata": {
        "id": "jsU3mg-ZwaS1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode('sarcastic,', return_tensors='tf')\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=128,\n",
        "    top_k=12,\n",
        "    top_p=0.82,\n",
        "    num_return_sequences=10\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yITurRNZ2mZ",
        "outputId": "baad22db-6d8c-4252-b044-594cad55107b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: sarcastic,Just finished the book 'The Magician's Shop' and it was a great read!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "1: sarcastic,@mikey_davies1 @RealMikeyDavies2 @RealMikeyDavies2 I’m sorry but I don’t understand the #GamerGate movement. I don’t understand the hate and negativity that has been directed at women and minorities in video games over the past year. I don’t understand the amount of time and effort that went into this #SuspendTheGamerGate hashtag and hashtag of women who were targeted by the #GamerGate hashtag. I don’t understand how anyone could support such a hate group and then\n",
            "2: sarcastic,@DumbAsInTheSky @jeffreykraus @jeremykraus @dumbasilves @TheDumbAsInTheSky I'm gonna die of a broken heart because I'm too fucking stupid to realize that I’m a fucking retard 😭!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "3: sarcastic,@TheDailyMail @Gizmodo We're not going to let the likes of @Gizmodo, @TheDailyMail & others ruin the fun of the #GDC #GDC18. #GDC18 #DC18 #GeekCon #DC18 #Geek #GeekCineAway #GeekTalk #GeekLife #GeekLife #GeekMom #GeekMom #GeekMom #GeekMom #GeekMom #GeekMom #GeekMom #GeekMom #GeekMom #GeekMom #GeekMom\n",
            "4: sarcastic,I’m glad I got my ass handed to me by a man I can’t wait to meet!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "5: sarcastic,@krispykrackers @TheWelshHammer @TheWelshHammer I think my mom is the worst. She’ll never understand a thing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "6: sarcastic,@TheHorseRaces @TheRealHerbst I’m going to be so grateful when the NFL makes an app that allows fans to watch live pre-game shows live from their living rooms!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "7: sarcastic,@RangersFC We've got some very good players on our books and they're not playing well. We need to address the squad!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "8: sarcastic,@david_kendricks I’ve never seen a dog that didn’t bark!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "9: sarcastic,I don’t understand the obsession with #Gamergate. #GamerGate has nothing to do with #GamerGate!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare model for download"
      ],
      "metadata": {
        "id": "50j9iYpyZ6Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r -X gpt-2_tuned.zip transformers/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R-1wVkDZ9cW",
        "outputId": "85ba30ac-7509-4396-b5e4-c1cc3edcc8fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: transformers/output/ (stored 0%)\n",
            "  adding: transformers/output/special_tokens_map.json (deflated 52%)\n",
            "  adding: transformers/output/generation_config.json (deflated 24%)\n",
            "  adding: transformers/output/model.safetensors (deflated 7%)\n",
            "  adding: transformers/output/config.json (deflated 52%)\n",
            "  adding: transformers/output/vocab.json (deflated 59%)\n",
            "  adding: transformers/output/tokenizer_config.json (deflated 51%)\n",
            "  adding: transformers/output/merges.txt (deflated 53%)\n",
            "  adding: transformers/output/tokenizer.json (deflated 72%)\n",
            "  adding: transformers/output/training_args.bin (deflated 50%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_utterances_df(n_generated, tokenizer, model, intent):\n",
        "  input_ids = tokenizer.encode(intent + ',', return_tensors='tf')\n",
        "  sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=GLOBAL_GENERATE_MAX_LENGTH,\n",
        "    top_k=n_generated,\n",
        "    top_p=GLOBALGENERATE_TOP_P,\n",
        "    num_return_sequences=n_generated\n",
        "  )\n",
        "\n",
        "  list_of_intent_and_utterances = [\n",
        "    (\n",
        "        intent,\n",
        "        tokenizer.decode(sample_output, skip_special_tokens=True)[len(intent)+1:]\n",
        "    )\n",
        "    for sample_output in sample_outputs\n",
        "  ]\n",
        "\n",
        "  return pd.DataFrame(list_of_intent_and_utterances, columns=['intent', 'utterance'])"
      ],
      "metadata": {
        "id": "uHuXMcR9Z_yt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like in Tavor et al. , generate ten-times more samples as you want to finally use as augmentations (ca. 5400)"
      ],
      "metadata": {
        "id": "TLZXlPiicPE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pd.read_csv(f'/content/train.csv')\n",
        "intents = data_train[\"intent\"].unique()\n",
        "\n",
        "generated_utterances_df = pd.DataFrame(columns=['intent', 'utterance'])\n",
        "while generated_utterances_df.shape[0] <= 5400:\n",
        "  print(generated_utterances_df.shape[0])\n",
        "  for intent in intents:\n",
        "    print(\"Generating for intent \" + intent)\n",
        "    if intent == \"sarcastic\":\n",
        "      utterances_for_intent_df = generate_utterances_df(NUMBER_OF_GENERATED_UTTERANCES_PER_INTENT, tokenizer, model, intent)\n",
        "      generated_utterances_df = pd.concat([generated_utterances_df, utterances_for_intent_df], ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmyjMaylcNwc",
        "outputId": "32d63e3d-66c0-4a5f-a66b-38e4dbdbf8f9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "300\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "600\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "900\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "1200\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "1500\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "1800\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "2100\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "2400\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "2700\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "3000\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "3300\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "3600\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "3900\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "4200\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "4500\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "4800\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "5100\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n",
            "5400\n",
            "Generating for intent not sarcastic\n",
            "Generating for intent sarcastic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_utterances_df.to_csv(\"generated.csv\", index=False)"
      ],
      "metadata": {
        "id": "Be42596ocxAs"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}